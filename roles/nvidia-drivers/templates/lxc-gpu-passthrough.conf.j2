# NVIDIA GPU passthrough configuration for LXC containers
# This file configures NVIDIA driver options for GPU passthrough
# Generated by Ansible for official NVIDIA network repository installation

# NVIDIA driver options for optimal LXC performance
options nvidia NVreg_DeviceFileUID={{ gpu_optimizations.lxc.device_file_uid }} NVreg_DeviceFileGID={{ gpu_optimizations.lxc.device_file_gid }} NVreg_DeviceFileMode={{ gpu_optimizations.lxc.device_file_mode }}

# Device access rules for LXC containers
# Add these lines to your LXC container configuration file:
# /etc/pve/lxc/<CONTAINER_ID>.conf
#
# GPU Device Access (cgroup2 rules):
{% for device in lxc_gpu_devices %}
# lxc.cgroup2.devices.allow: {{ device }}
{% endfor %}
#
# Device Mounts (bind mount GPU devices into container):
{% for device_node in nvidia_device_nodes %}
{% if device_node != '/dev/nvidia1' and device_node != '/dev/nvidia2' and device_node != '/dev/nvidia3' %}
# lxc.mount.entry: {{ device_node }} {{ device_node | replace('/dev/', 'dev/') }} none bind,optional,create=file
{% endif %}
{% endfor %}
#
# Container Features:
# features: nesting=1
#
# Example complete LXC configuration for GPU passthrough:
# Copy the following lines to your LXC container configuration:
# /etc/pve/lxc/<CONTAINER_ID>.conf
#
# GPU Device Access
# lxc.cgroup2.devices.allow: c 195:* rwm
# lxc.cgroup2.devices.allow: c 235:* rwm
# lxc.cgroup2.devices.allow: c 509:* rwm
#
# Device Mounts
# lxc.mount.entry: /dev/nvidia0 dev/nvidia0 none bind,optional,create=file
# lxc.mount.entry: /dev/nvidiactl dev/nvidiactl none bind,optional,create=file
# lxc.mount.entry: /dev/nvidia-uvm dev/nvidia-uvm none bind,optional,create=file
# lxc.mount.entry: /dev/nvidia-uvm-tools dev/nvidia-uvm-tools none bind,optional,create=file
# lxc.mount.entry: /dev/nvidia-modeset dev/nvidia-modeset none bind,optional,create=file
#
# Features
# features: nesting=1

# After adding these configurations:
# 1. Restart the LXC container: pct restart <CONTAINER_ID>
# 2. Install NVIDIA drivers inside the container
# 3. Test GPU access: nvidia-smi (inside container)
# 4. For Docker workloads: install nvidia-container-toolkit inside container